---
title: "Fooled by correlation"
description: |
  How to not get fooled by correlation using Bayesian stats
author:
  - name: Miha Gazvoda
    url: https://mihagazvoda.com
date: 08-15-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r, echo=FALSE}
# library(lemon)
# knit_print.data.frame <- lemon_print
# knit_print.table <- lemon_print
# knit_print.grouped_df <- lemon_print 
# knit_print.tibble <- lemon_print
# knit_print.tbl <- lemon_print
options(digits=2)
ggplot2::theme_set(ggplot2::theme_classic())
```


Once upon a time I worked as a student data scientist for a start-up where we wanted to predict a person's biomarker based on other biomarkers that were easier to measure. In theory, these measurements should correlate with the biomarker we wanted to predict.

> In theory, theory and practice are the same. In practice, they are not. - Unknown

Our data consisted of, let's say, 30 subject, each with 20 features that were extracted from measured biomarkers and the biomarker we wanted to predict. We often found correlations, got all hyped up, and disappointed after they disappeared on the new subject's data. I suspected that we were getting correlations just by chance. I knew this happens but to what degree? Would it be possible to get so called [moderate or even high degree](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/pearsons-correlation-coefficient/) of spurious correlation? My plan was to do a simulation to check that. I left the job before I got to do the simulation.

Until, a few weeks ago, I came across [the video](https://wwwyoutube.com/watch?v=fb921ZrM6h0) by Nassim Nicholas Taleb, doing and explaining the thing I planned to do.

## How to fool yourself with correlation

Let's simulate the situation we were probably in. Let's say we measured 30 subjects and then calculated 20 features for each one. All features, including the outcome, are random^[Normally distributed with the mean 0 and standard deviation 1.] and independent.

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

n_features <- 20
n_subjects <- 30

subject_data <- as_tibble(
  replicate(n_features + 1, rnorm(n_subjects)), 
  .name_repair = ~c(paste0("x", as.character(1:n_features)), "y")
)

subject_data
```

As we can see above, each row presents one subject. Columns `x1` to `x20` present features and `y` the value we want to predict. Let's calculate [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), also known as `r`, between each feature and `y`. In reality, they should be zero, but are they? 

```{r}
corrs_with_y <- subject_data %>% 
  corrr::correlate(method = "pearson") %>%
  corrr::focus(y) %>% 
  rename(feature = "term", r = "y") %>% 
  arrange(desc(abs(r)))

corrr::fashion(corrs_with_y)
```

We are getting decent correlations out of a thin air! Look at the plots below, displaying 3 best spurious correlations.  


```{r}
specify_decimal <- function(x, k) trimws(format(round(x, k), nsmall=k))

features_with_highest_corrs <- corrs_with_y %>% 
  head(3)

subject_data %>%
  pivot_longer(cols = starts_with("x"), names_to = "feature", values_to = "x") %>%
  right_join(features_with_highest_corrs, by = "feature") %>% 
  mutate(label = paste0(feature, ": ", specify_decimal(r, 2))) %>% 
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_grid(~forcats::fct_reorder(label, desc(abs(r))))
```

We often see something like this in the news, even scientific publications! A person analyzing the data produces multiple correlations and then [cherry picks](https://en.wikipedia.org/wiki/Cherry_picking) the best ones that signal that she found something meaningful. For example, one could correlate impact of different foods on life longevity. If your dataset consists from a lot of foods, you will for sure find a correlation. But this correlation won't be causation. It won't even be correlation!

> The point is not that the correlation is not causation, is that very often correlation is not correlation. - Nassim Nicholas Taleb

Metrics, including correlations, are random variables. Since people have an incentive (in science: [publish the results](https://en.wikipedia.org/wiki/Publication_bias)) they will (often unintentionally) game the metric by finding the higher bound of it. 

## Simulation 1: Getting spurious correlations by chance

How often can independent data produce good-looking correlations? Let's look at the distribution of correlations of two independent datasets with `{r} n_subjects` data points.

```{r}
simulated_correlations <- replicate(10000, cor(rnorm(n_subjects), rnorm(n_subjects)))

tibble(r = simulated_correlations) %>% 
  ggplot(aes(r)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(simulated_correlations), sd = sd(simulated_correlations))
  )
```

This looks like the most normal histogram I've seen, the normal function fits perfectly. See the table below what's the probability to get correlation higher than some value.

```{r}
tibble(
  simulated_correlations = list(simulated_correlations), 
  r_threshold = seq(0.1, 0.5, by = 0.1)
) %>%
  rowwise() %>% 
  mutate(p_above_threshold = mean(abs(simulated_correlations) > r_threshold)) %>% 
  ungroup() %>% 
  select(r_threshold, p_above_threshold)
```

That's not little! Remember the time when we were all enthusiastic about 0.4 correlation in data. Let's calculate how standard deviation and 95th percentile of correlation coefficient distribution change with sample size.

```{r}
df <- crossing(
  n = seq(5, 100, 5),
  k = 1:1000
) %>% 
  rowwise() %>% 
  mutate(r = cor(rnorm(n), rnorm(n))) 

df %>% 
  group_by(n) %>% 
  summarise(
    sd = sd(r), 
    p95 = quantile(abs(r), 0.95)
  ) %>% 
  ggplot(aes(x = n)) + 
  geom_line(aes(y = sd)) + 
  geom_line(aes(y = p95), linetype = "dashed") + 
  expand_limits(y = 0)
```

<!-- The values are dropping as $sqrt(n)$.  -->

Since we know the mean and standard deviation describing distribution of pearson correlation coefficients we could use it for hypotesis testing. `p` value would tell you probability that such or more extreme `r` could be generated if there's no correlation and then reject the null hypothesis that it comes from distribution where actual r = 0 if p < 0.05. 

But this might not be the answer we are looking for. What if we are looking for `P(r > some value|observed r)`? What would be the distributed of true r values given our observed one? Once again, Bayes to the rescue!

## Simulation 2: Bayesian remedy for fake correlations

Right now, we were calculating likelihood distribution of $P( r_{obs} | r_{true})$. But we are interested in posterior $P( r_{true} | r_{obs})$ - what's the distribution of actual $r$ given you observed `r=0.5`, for example?

First, we match all combinations of sample size `n`, `r_true`, actual correlation coefficient, and `k`, index of simulation. For each row we generate vector of values `x` that has a length `n`. `y` is generated from `x` and `r_true` with the `rnorm_pre` from [faux](https://debruine.github.io/faux/) package. This function makes a normal vector (`y`) that is correlated to existing vector (`x`). But here's the trick, although it should be correlated with `r_true`, the actual correlation varies - so we calculate it again from `x` and `y` to get `r_obs`. 

```{r}
correlations_positive_only <- crossing(
  n = as.integer(c(10, 100, 1000)),
  k = 1:10,
  r_true = seq(0, 0.9, by = 0.05)
) %>% 
  rowwise() %>% 
  mutate(
    x = list(rnorm(n)),
    y = list(faux::rnorm_pre(x, r = r_true)),
    r_obs = cor(x, y)
  ) %>% 
  ungroup() %>% 
  select(-x, -y)

df <- bind_rows(
    correlations_positive_only,
    correlations_positive_only %>% 
      filter(r_true > 0) %>%
      mutate(across(c(r_true, r_obs), `-`))
  )

head(df)
```
So for example, to get the same of $P( r_{obs} | r_{true})$ as we had before for $r_{true}=0$, now we can calculate that only from filtering `df` with appropriate $r_{true}$ and sample size `n`.

```{r}
df %>% 
    filter(r_true == 0.5, n == 100) %>% 
    ggplot(aes(r_obs)) + 
    geom_histogram(bins = 20)
```

Let's go to Bayesian formula again. $$P(r_{true} | r_{obs}) = P(r_{true}) P( r_{obs} | r_{true})$$. As usual, we will drop the bottom part. First for each $n$ and $r_{true}$ what's the probability it will generate $r_{obs}$. We bin $r_{obs}$ so we can actually calculate probabilities for it. This technique is called grid approximation - you can read more about it on my [Intuitive Bayesian Inference](https://mihagazvoda.com/posts/2020-11-24-bayesian-updating/) blog post. 
TODO Also mention priors.

```{r}
tmp <- df %>% 
  group_by(n, r_true, r_obs_rounded = plyr::round_any(r_obs, 0.1)) %>% 
  summarise(count = n()) %>% 
  mutate(likelihood = count / sum(count)) %>% 
  ungroup() %>% 
  mutate(
    prior = dnorm(r_true, x = 0, sd = 0.5),
    posterior_unstd = likelihood * prior
  ) %>% 
  group_by(n, r_obs_rounded) %>% 
  mutate(posterior = posterior_unstd / sum(posterior_unstd))

tmp
```



```{r}
tmp %>% 
  filter(as.character(r_obs_rounded) %in% as.character(c(0.3, 0.5, 0.7))) %>% 
  ggplot(aes(r_true, posterior, color = factor(n))) + 
  geom_line() +
  facet_grid(~r_obs_rounded) + 
  scale_color_viridis_d()
```





mention that this also holds for other metrics, not only correlation

the lesson is: 1. metrics are random variables. If they are random variables, they will be gamed. 
