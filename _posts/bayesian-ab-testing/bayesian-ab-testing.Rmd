---
title: "Bayesian AB Testing"
description: |
  A short description of the post.
author:
  - name: Miha Gazvoda
    url: https://mihagazvoda.com
date: 04-17-2021
categories: 
  - R
  - data science
  - Bayesian
output:
  distill::distill_article:
    self_contained: false
---


In this post I will introduce you to Bayesian A/B testing for Binomial data (proportions of clicks on an ad or heads when throwing a coin). We will start with the  

# Introduction

In one of my previous posts I talked about [Intuitive Bayesian Inference](https://mihagazvoda.com/posts/2020-11-24-bayesian-updating/) for Bernoulli trials. The focus was about building intuition on Bayesian inference. Here we will take a look on how to use Beta function to make the Bayesian inference for Binomial data smoother. 
In this example, let's say we want to figure out who's better basketball 3-point shooter. Again, we will have data that it will look like: 

```{r}
made_3_point <- c(FALSE, FALSE, FALSE)
```

The skill is not changing over time. We want to figure out what is my true shot precision.

# Conjugate

We could use Bayesian rule to figure out this: 

$$ 
P(p|data) = \frac{P(data|p)P(p)}{P(data)}
$$
where $P(p|data)$ is the (updated) posterior distribution of my skill $p$ given likelihood $P(data|p)$ and prior assumptions `P(p)` and `P(data)` about my skill and data.

$$
posterior \propto likelihood \times prior
$$
This is not to complicated but there exist conjugate priors that will make this even simpler. 

In Bayesian probability theory, if the posterior distributions p(θ | x) are in the same probability distribution family as the prior probability distribution p(θ), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function p(x | θ).

For Bernoulli trials - this conjugate distribution is beta.

# Beta function, the alpha (wo)man of Binomial Bayesian model

[Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution) says that the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by $\alpha$ and $\beta$. 

That's great since our `p` (probability of hitting a 3-pointer) is also on this range. We need just update the parameters $\alpha$ and $\beta$ based on observed data to come from prior to posterior. But how?

Parameters present number of trial successes ($\alpha$) and failures ($\beta$). Mean of the distribution (expected $p$) can be calculated as:^[Variance: $var[X] = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$] 
$$E[X] = \frac{\alpha}{\alpha + \beta}$$
So beta functions with different parameters look like this:

```{r}
library(dplyr)
library(ggplot2)

tidyr::crossing(
  alpha = 1:2, 
  beta = c(1, 5, 9), 
  x = seq(0, 1, length.out = 100)
) %>% 
  mutate(
    y = dbeta(x, alpha, beta),
    parameter = paste(alpha, beta)
  ) %>% 
  ggplot(aes(x, y, color = parameter)) + 
  geom_line() +
  viridis::scale_color_viridis(discrete = TRUE) + 
  theme_classic()
```









# Plan: 

* how it relates to my previous post
* introduction to conjugate priors
* more about beta

* a simple example with beta

```{r}
source("functions.R")
prop_model(c(TRUE, FALSE, TRUE))
```

* why would you prefer bayesian over standard testing

> First of all, interpretability is everything. Would you rather say "P(A > B) is 10%", or "Assuming the null hypothesis that A and B are equal is true, the probability that we would see a result this extreme in A vs B is equal to 3%"?`

* repeated testing?

* compare standard vs bayesian CI




* some tool to play around with betas

* play around with priors

 w

# Introduction


# Resources
Blogs: 

* https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing
* http://fportman.com/writing/bayesab-a-new-r-package-for-bayesian-ab-testing/
* http://varianceexplained.org/r/bayesian-ab-testing/
* beautiful viz: http://www.sumsar.net/blog/2018/12/visualizing-the-beta-binomial/




